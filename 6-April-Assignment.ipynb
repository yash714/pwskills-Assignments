{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386433b-1a48-43de-a8b4-7368b41728ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51d19c-2e90-4978-bf6d-c64467c40101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A linear support vector machine (SVM) is a binary classification algorithm that seeks to find the best separating hyperplane between two classes of data points. The mathematical formula for a linear SVM is as follows:\n",
    "\n",
    "Given a training set of n data points {(x₁, y₁), (x₂, y₂), ..., (xn, yn)}, where xi is the i-th input vector and yi is its corresponding output class label, which can be either -1 or 1, the goal of a linear SVM is to find a hyperplane that maximizes the margin between the two classes. The hyperplane is represented by the equation:\n",
    "\n",
    "w · x + b = 0\n",
    "\n",
    "where w is a vector perpendicular to the hyperplane, x is an input vector, and b is the bias term.\n",
    "\n",
    "The distance between the hyperplane and the closest data point from each class is called the margin. The optimal hyperplane is the one that maximizes the margin.\n",
    "\n",
    "To find the optimal hyperplane, the SVM algorithm solves the following optimization problem:\n",
    "\n",
    "minimize 1/2 ||w||²\n",
    "subject to yi(w · xi + b) ≥ 1 for i = 1, 2, ..., n\n",
    "\n",
    "where ||w|| is the Euclidean norm of the weight vector w.\n",
    "\n",
    "The above optimization problem is a convex quadratic programming problem, which can be efficiently solved using various optimization techniques, such as gradient descent or quadratic programming. Once the optimal hyperplane is found, new data points can be classified by evaluating which side of the hyperplane they fall on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d17465-9ecc-4faa-86c1-594892f874cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855bb58-459b-4189-8ef6-de3aa9c6b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the hyperplane that maximizes the margin between the two classes in a binary classification problem. The margin is defined as the distance between the hyperplane and the closest data points from each class.\n",
    "\n",
    "The objective function of a linear SVM is a convex quadratic optimization problem. Given a training set of n data points {(x₁, y₁), (x₂, y₂), ..., (xn, yn)}, where xi is the i-th input vector and yi is its corresponding output class label, which can be either -1 or 1, the objective function of a linear SVM is to minimize:\n",
    "\n",
    "1/2 ||w||²\n",
    "\n",
    "subject to yi(w · xi + b) ≥ 1 for i = 1, 2, ..., n\n",
    "\n",
    "where ||w|| is the Euclidean norm of the weight vector w, and b is the bias term. The inequality constraint ensures that all data points are correctly classified with a margin of at least 1.\n",
    "\n",
    "This optimization problem seeks to find the hyperplane that maximizes the margin between the two classes. The margin is proportional to 1/||w||, so by minimizing ||w||, we maximize the margin. This is equivalent to finding the hyperplane that best separates the two classes while minimizing the risk of misclassification.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e7e11-caa2-48ee-bc0d-83840f1ece0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25983073-7b8a-4dd4-a25f-7306d5cffc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The kernel trick is a technique used in Support Vector Machine (SVM) algorithms to efficiently handle nonlinearly separable data by mapping the data into a higher-dimensional feature space. In the kernel trick, we do not need to explicitly transform the data into a higher-dimensional space, but instead, we use a kernel function to compute the inner products of the data points in the higher-dimensional space.\n",
    "\n",
    "A kernel function is a function that takes two input vectors and returns their inner product in the higher-dimensional space. The kernel function effectively measures the similarity between two data points in the feature space. The most commonly used kernel functions are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel.\n",
    "\n",
    "By using a kernel function, we can avoid the expensive computation required to explicitly transform the data into a higher-dimensional space. This is because the kernel function computes the inner products in the feature space directly from the original data points, without ever explicitly computing the coordinates of the data points in the feature space.\n",
    "\n",
    "The kernel trick allows SVMs to efficiently handle large amounts of high-dimensional data, even when the data is nonlinearly separable. By using a kernel function, we can find a hyperplane in the feature space that separates the data points with maximum margin, even when the data is not linearly separable in the original input space.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873aaf21-08a1-4444-aac8-43068acfc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b61ed-d587-4251-a823-ff143e223aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In a Support Vector Machine (SVM) algorithm, support vectors are the data points that lie closest to the decision boundary or the hyperplane. Support vectors are critical to the SVM algorithm because they define the decision boundary and determine the margin of the hyperplane.\n",
    "\n",
    "In other words, the support vectors are the data points that determine the optimal hyperplane and the maximum margin in the SVM algorithm. Any data points that are not support vectors do not affect the decision boundary and are not used in the classification process.\n",
    "\n",
    "Here's an example to illustrate the role of support vectors in SVM:\n",
    "\n",
    "Suppose we have a binary classification problem where we want to classify emails as spam or non-spam based on the presence of certain keywords. We have a training set of 100 emails, where 40 are labeled as spam and 60 are labeled as non-spam. We use a linear SVM to find the decision boundary between the two classes.\n",
    "\n",
    "After training the SVM, we obtain the following decision boundary:\n",
    "\n",
    "w · x + b = 0\n",
    "\n",
    "where w is the weight vector, x is the input vector, and b is the bias term.\n",
    "\n",
    "In the input space, this decision boundary is a line that separates the two classes. However, in the feature space, the decision boundary is a hyperplane that separates the two classes with maximum margin.\n",
    "\n",
    "The support vectors are the data points that lie closest to the decision boundary or the hyperplane. Let's say that we have 10 support vectors in our example, 5 from the spam class and 5 from the non-spam class. These support vectors define the optimal hyperplane and the maximum margin.\n",
    "\n",
    "Any new email that we want to classify is mapped into the feature space using the same kernel function used in the training process. We then evaluate which side of the hyperplane the email falls on to determine its class label.\n",
    "\n",
    "In summary, the support vectors play a crucial role in SVM because they define the decision boundary and determine the maximum margin. They are the data points that are closest to the hyperplane and are used to classify new data points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769b907-6353-4ee2-a973-7b87f67ef563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19259d-16e3-4580-a344-9e9d2a63f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperplane: In an SVM, a hyperplane is a decision boundary that separates the two classes in a binary classification problem. The hyperplane can be linear or nonlinear and is represented as a line or a curve in the input space. In a linear SVM, the hyperplane is a linear function that separates the two classes in the input space. \n",
    "\n",
    "Marginal plane: In an SVM, the marginal plane is a parallel hyperplane that runs through the support vectors and is equidistant from the hyperplane that separates the two classes. The distance between the marginal plane and the hyperplane is called the margin. The marginal plane is used to define the maximum margin that separates the two classes in a binary classification problem. \n",
    "\n",
    "Soft margin: In an SVM, the soft margin allows some misclassification of data points in order to achieve a wider margin and a more flexible decision boundary. Soft margin is used when the data is not linearly separable, and it allows some data points to be misclassified in order to find a hyperplane that separates the two classes with a larger margin. \n",
    "\n",
    "Hard margin: In an SVM, the hard margin does not allow any misclassification of data points and is used when the data is linearly separable. Hard margin seeks to find a hyperplane that separates the two classes without any misclassification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71183d34-bfc9-4a95-84d1-7260fb3a3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6.  \n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "svm_clf.fit(X_train[:, (2, 3)], y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test[:, (2, 3)])\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the features\n",
    "x_min, x_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
    "y_min, y_max = X[:, 3].min() - 1, X[:, 3].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 2], X[:, 3], c=y, alpha=0.8)\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.show()\n",
    "\n",
    "# Try different values of the regularisation parameter C and see how it affects the performance of the model\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    svm_clf = LinearSVC(C=C, loss=\"hinge\", random_state=42)\n",
    "    svm_clf.fit(X_train[:, (2, 3)], y_train)\n",
    "    y_pred = svm_clf.predict(X_test[:, (2, 3)])\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy (C={}): {}\".format(C, accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2df80-c99f-4b42-8a02-6c6cb1eadcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3858b89-4cc1-411e-a7e4-96df59156518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab278c1-d585-4d3e-9629-534489397ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c5e2a-0eb6-41d7-8413-da06224177c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
