{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb112142-9e2f-4f53-a746-11272f010f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
    "categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
    "m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "eng#neer#ng process and handles the m#ss#ng valuesD\n",
    "Des#gn a p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Use an automated feature select#on method to #dent#fy the #mportant features #n the datasetC\n",
    "Create a numer#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the numer#cal columns us#ng the mean of the column valuesC\n",
    "Scale the numer#cal columns us#ng standard#sat#onC\n",
    "Create a categor#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the categor#cal columns us#ng the most frequent value of the columnC\n",
    "One-hot encode the categor#cal columnsC\n",
    "Comb#ne the numer#cal and categor#cal p#pel#nes us#ng a ColumnTransformerC\n",
    "Use a Random Forest Class#f#er to bu#ld the f#nal modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Note! Your solut#on should #nclude code sn#ppets for each step of the p#pel#ne, and a br#ef explanat#on of\n",
    "each step. You should also prov#de an #nterpretat#on of the results and suggest poss#ble #mprovements for\n",
    "the p#pel#neD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80266a-45a7-4ea2-a22c-0e3c335094f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.Feature Selection: Use an automated feature selection method such as SelectKBest or Recursive Feature Elimination (RFE) to identify the important features in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7e8e3-73c8-400a-bd11-8e909255d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define feature selection and classification pipeline\n",
    "feature_selection_pipeline = Pipeline([\n",
    "    ('select_k_best', SelectKBest(score_func=f_classif, k=10))\n",
    "])\n",
    "\n",
    "# Select the important features from the dataset\n",
    "X_train_selected = feature_selection_pipeline.fit_transform(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33aae71-2796-45e5-a012-6d0182f7feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Numerical Pipeline: Create a numerical pipeline that imputes missing values in the numerical columns with the mean of the column values and scales the numerical columns using standardization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391b8f3-82f4-4438-b9f0-dab325b74ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply numerical pipeline to numerical columns in dataset\n",
    "numerical_features = ['numerical_feature_1', 'numerical_feature_2', ...]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Transform the numerical columns in the dataset\n",
    "X_train_processed = preprocessor.fit_transform(X_train_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dd0d3-3729-4c58-a068-911302d42218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Categorical Pipeline: Create a categorical pipeline that imputes missing values in the categorical columns with the most frequent value of the column and one-hot encodes the categorical columns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43a75f-f0c0-4b65-b922-f1f954bf1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Apply categorical pipeline to categorical columns in dataset\n",
    "categorical_features = ['categorical_feature_1', 'categorical_feature_2', ...]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Transform the categorical columns in the dataset\n",
    "X_train_processed = preprocessor.fit_transform(X_train_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e29e8-cd01-4c33-8fd2-8c1dc386ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.Combine the Numerical and Categorical Pipelines: Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9203833-171d-43b2-a4ef-5dc2098425ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Transform the numerical and categorical columns in the dataset\n",
    "X_train_processed = preprocessor.fit_transform(X_train_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4007bfb-b656-4de3-913e-1695cc73ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Random Forest Classifier: Use a Random Forest Classifier to build the final model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdd1f3-4f83-47bf-8c85-35646415f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest Classifier model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Train the Random Forest Classifier model\n",
    "rf_classifier.fit(X_train_processed, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd27d1c-e9d3-4ee2-9391-ed515b3fd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Model Evaluation: Evaluate the accuracy of the model on the test dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34fd40-7d9f-4378-a804-8d34cd1afa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and preprocess the test dataset using the preprocessor\n",
    "X_test_selected = feature_selection_pipeline.transform(X_test)\n",
    "X_test_processed = preprocessor.transform(X_test_selected)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy = rf_classifier.score(X_test_processed, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e757dd-c065-41b4-a834-a6b864cad9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. Bu#ld a p#pel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then\n",
    "use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts\n",
    "accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002ac57-a038-4525-a91a-0a1ae335261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps for the numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the preprocessing steps for the categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define the preprocessor to be used in the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, make_column_selector(dtype_include=np.number)),\n",
    "    ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n",
    "])\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Define the Logistic Regression Classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the Voting Classifier to combine the predictions of the Random Forest Classifier and the Logistic Regression Classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rfc', rfc),\n",
    "    ('lr', lr)\n",
    "], voting='hard')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('voting_clf', voting_clf)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
