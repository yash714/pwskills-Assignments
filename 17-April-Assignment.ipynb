{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bf61c-a699-4ed9-8a74-87f68fb78b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What is Gradient Boosting Regression?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d1e6b-0eb1-49b9-8727-e7095d216e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting Regression is a machine learning technique that uses an ensemble of decision trees to predict a numerical output, such as a continuous variable. It works by iteratively building a sequence of decision trees, where each new tree is trained to predict the residual error of the previous trees.\n",
    "\n",
    "The \"Gradient\" in Gradient Boosting refers to the optimization algorithm used to minimize the loss function. It works by calculating the negative gradient of the loss function with respect to the predicted value at each step, and using this information to update the model parameters in the direction that reduces the loss.\n",
    "\n",
    "The \"Boosting\" in Gradient Boosting refers to the fact that each new tree is trained to correct the errors of the previous trees. This iterative process continues until a stopping criterion is met, such as a maximum number of trees or a minimum improvement in performance.\n",
    "\n",
    "Gradient Boosting Regression is a powerful technique that has been shown to perform well on a wide range of regression problems, including those with noisy or nonlinear data. It is also relatively easy to implement and has a number of hyperparameters that can be tuned to improve performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71d88-6eee-41fe-ad1b-f4818195edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f35a45d-9b78-4cfb-ad9c-a8c7f8cae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "# Define loss function (mean squared error)\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define gradient of loss function\n",
    "def mse_gradient(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize prediction to be the mean of the training labels\n",
    "prediction = np.mean(y_train)\n",
    "\n",
    "# Train the model\n",
    "for i in range(100):\n",
    "    # Compute residuals\n",
    "    residuals = mse_gradient(y_train, prediction)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d99091-5f9d-4ead-84a8-7ebba52d425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b468f-2e8b-4883-9280-3de55fb6103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "# Define loss function (mean squared error)\n",
    "def mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Define gradient of loss function\n",
    "def mse_gradient(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n",
    "\n",
    "# Define grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Define gradient boosting regressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gbr, param_grid, cv=5, n_jobs=-1, verbose=1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", -grid_search.best_score_)\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mse(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE = {test_mse:.4f}, Test R^2 = {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37184288-4d60-452c-8d44-33085b88014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ccec3-49e7-40f3-8fbc-89c2c0ad3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A weak learner in Gradient Boosting is a model that performs only slightly better than random guessing, but still provides some predictive power. In the context of Gradient Boosting, the weak learner is typically a decision tree with a small number of nodes (also called a \"shallow\" tree). These shallow decision trees are trained on subsets of the data and are combined to form a stronger predictive model.\n",
    "\n",
    "The idea behind using weak learners in Gradient Boosting is to iteratively improve the model by fitting new trees to the errors made by the previous trees. In each iteration, the weak learner is trained on the residuals (i.e., the differences between the true values and the predictions of the previous model), so that it learns to correct the mistakes made by the previous model. By doing this repeatedly, the model gradually improves its predictive power, as the combination of weak learners can capture more complex patterns in the data.\n",
    "\n",
    "In summary, the use of weak learners in Gradient Boosting allows the model to be trained in an iterative and adaptive way, improving its performance at each iteration by focusing on the most difficult examples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb530a4-616c-40a8-8532-aacceb8cd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b3a79-f436-45ca-8dbc-ca022342828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The intuition behind Gradient Boosting algorithm is to create a strong model by combining multiple weak models in an iterative and adaptive way. The idea is to improve the model by fitting new models to the errors made by the previous models.\n",
    "\n",
    "At the beginning of the algorithm, we start with a simple model (often a decision tree with just a few splits) and fit it to the data. We then calculate the residuals (i.e., the differences between the predicted values and the true values) and fit a new model to the residuals. The second model aims to correct the errors made by the first model. We repeat this process, fitting new models to the residuals of the previous models, until the residuals are too small to be significant or we reach a pre-specified maximum number of iterations.\n",
    "\n",
    "Each new model is trained on the residuals of the previous models and added to the ensemble using a weight that is determined by a learning rate parameter. The learning rate controls the contribution of each new model to the final prediction, allowing us to prevent overfitting and achieve a better generalization performance.\n",
    "\n",
    "The final prediction is obtained by summing the predictions of all the models in the ensemble, each weighted by its contribution to the final prediction. This results in a model that is much more accurate and robust than any of the individual models used in the ensemble.\n",
    "\n",
    "In summary, the Gradient Boosting algorithm works by iteratively adding weak models to an ensemble, with each new model aimed at correcting the errors made by the previous models. The algorithm learns from its mistakes and focuses on the most difficult examples, leading to a more accurate and robust model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd2298-525a-4f3f-9423-ab9803db6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595ef54-c953-4ad3-897f-405c0dfd4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the ensemble, each of which corrects the errors made by the previous models. The process of building the ensemble can be summarized as follows:\n",
    "\n",
    "The first model in the ensemble is typically a simple model, such as a decision tree with just a few splits, that is trained on the original dataset.\n",
    "\n",
    "In each subsequent iteration, a new model is trained on the residuals (i.e., the differences between the predicted values and the true values) of the previous models. The aim of the new model is to correct the errors made by the previous models.\n",
    "\n",
    "The new model is then added to the ensemble using a weight that is determined by a learning rate parameter. The learning rate controls the contribution of each new model to the final prediction, allowing us to prevent overfitting and achieve a better generalization performance.\n",
    "\n",
    "The process of adding new models to the ensemble is repeated for a pre-specified number of iterations or until the residuals are too small to be significant.\n",
    "\n",
    "The final prediction is obtained by summing the predictions of all the models in the ensemble, each weighted by its contribution to the final prediction.\n",
    "\n",
    "By adding new models to the ensemble in an iterative and adaptive way, the Gradient Boosting algorithm is able to create a powerful and robust model that is able to capture complex patterns in the data. The use of weak learners and the focus on the most difficult examples allows the algorithm to learn from its mistakes and achieve a high level of accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef325a-fec1-417a-b409-1f8a75a26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e41f1-8141-409a-b97a-9dc2d9576e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The mathematical intuition behind the Gradient Boosting algorithm can be broken down into the following steps:\n",
    "\n",
    "Initialize the model: We start by defining a simple model that can make predictions on the input data. This could be a single decision tree with just a few splits, or a linear regression model.\n",
    "\n",
    "Define the loss function: We choose a loss function that measures the difference between the predicted values and the true values. For regression problems, the most common loss function is the mean squared error (MSE), which measures the average squared difference between the predicted and true values.\n",
    "\n",
    "Fit the model to the data: We fit the initial model to the data by minimizing the loss function. This gives us a set of initial predictions on the input data.\n",
    "\n",
    "Compute the residuals: We calculate the residuals (i.e., the differences between the true values and the initial predictions) for each example in the training data.\n",
    "\n",
    "Train a new model on the residuals: We fit a new model to the residuals by minimizing the loss function again. This new model aims to correct the errors made by the previous model.\n",
    "\n",
    "Add the new model to the ensemble: We add the new model to the ensemble, using a weight that is determined by a learning rate parameter. The learning rate controls the contribution of each new model to the final prediction, allowing us to prevent overfitting and achieve a better generalization performance.\n",
    "\n",
    "Repeat steps 4-6: We repeat steps 4-6 for a pre-specified number of iterations or until the residuals are too small to be significant.\n",
    "\n",
    "Obtain the final prediction: The final prediction is obtained by summing the predictions of all the models in the ensemble, each weighted by its contribution to the final prediction.\n",
    "\n",
    "By following these steps, the Gradient Boosting algorithm is able to iteratively improve the model by fitting new models to the residuals of the previous models. This allows the algorithm to learn from its mistakes and focus on the most difficult examples, leading to a more accurate and robust model.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
