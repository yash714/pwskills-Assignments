{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704df2eb-94d3-4b85-9882-6e4aa93cab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize\n",
    "the weights carefully\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94161643-ad47-45a6-9e32-d9489b8ff752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Weight initialization is a crucial step in training artificial neural networks. It involves setting the initial values for the weights of the network connections. The choice of initial weights can significantly impact the learning process and the performance of the neural network. Here are a few reasons why weight initialization is important:\n",
    "\n",
    "Avoiding the vanishing and exploding gradients problem: During the training process of a neural network, gradients are computed and used to update the weights. If the weights are initialized too small, it can lead to vanishing gradients, where the gradients become extremely small, and the network fails to learn. On the other hand, if the weights are initialized too large, it can result in exploding gradients, causing instability during training. Proper weight initialization helps mitigate these problems by ensuring that the gradients remain in a reasonable range.\n",
    "\n",
    "Breaking symmetry: Symmetry refers to the situation when multiple neurons in the same layer have the same weights and biases. This symmetry can cause all the neurons to update in the same way, making them redundant and limiting the learning capacity of the network. By carefully initializing the weights with random values, we can break this symmetry, allowing each neuron to learn different features from the input data.\n",
    "\n",
    "Efficient convergence: Proper weight initialization can help the network converge faster during training. If the weights are initialized poorly, it can result in a slow convergence or the network getting stuck in suboptimal solutions. Initializing the weights in an appropriate manner can provide a good starting point for the optimization process, allowing the network to converge more efficiently towards the desired solution.\n",
    "\n",
    "Generalization and avoiding overfitting: Weight initialization can affect the generalization ability of the neural network. If the weights are initialized too large, the network might overfit the training data, performing poorly on unseen data. Careful weight initialization techniques, such as regularization methods, can help prevent overfitting and improve the network's ability to generalize to new examples.\n",
    "\n",
    "It is necessary to initialize the weights carefully in the following scenarios:\n",
    "\n",
    "Deep neural networks: Deep networks with many layers are particularly sensitive to weight initialization. Since the gradients need to flow through multiple layers during backpropagation, improper initialization can lead to vanishing or exploding gradients, making training difficult. Special initialization techniques like Xavier or He initialization are commonly used for deep networks.\n",
    "\n",
    "Nonlinear activation functions: When using nonlinear activation functions like ReLU, sigmoid, or tanh, it is crucial to initialize the weights properly. If the weights are not initialized carefully, the activations can quickly saturate or become too large, impairing the network's ability to learn effectively.\n",
    "\n",
    "Transfer learning: In transfer learning, where pre-trained weights from one task are used as an initialization for a different task, careful weight initialization becomes important. Initializing the weights too randomly or differently from the pre-trained weights can lead to poor transfer performance. Techniques like fine-tuning or gradual unfreezing are often employed to adapt the pre-trained weights to the new task.\n",
    "\n",
    "In summary, weight initialization plays a significant role in the training and performance of neural networks. Proper initialization techniques can help avoid convergence issues, improve training speed, and enhance the network's ability to generalize and learn meaningful representations from the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227d252-33d4-48a9-b0b8-429ccf464877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Describe the challenges associated with improper weight initialization. How do these issues affect model\n",
    "training and convergenceD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8b3c2-27c0-4083-b467-168714972783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improper weight initialization can introduce several challenges during model training and convergence. Here are some of the issues that can arise:\n",
    "\n",
    "Vanishing and exploding gradients: If the weights are initialized poorly, such as setting them too small or too large, it can lead to vanishing or exploding gradients. Vanishing gradients occur when the gradients become extremely small as they propagate through the network, making it difficult for the model to learn effectively. On the other hand, exploding gradients occur when the gradients become very large, causing instability and making it challenging to find an optimal solution. Both cases can hinder model convergence and degrade training performance.\n",
    "\n",
    "Slow convergence or getting stuck in local optima: Poor weight initialization can lead to slow convergence during training. The model may require a large number of iterations to reach a satisfactory solution, slowing down the learning process. Moreover, improper initialization can cause the model to get stuck in local optima, where it fails to find the global optimum or a good solution for the given problem. This issue is particularly critical in complex optimization landscapes, such as deep neural networks, where finding the global optimum is challenging.\n",
    "\n",
    "Symmetry breaking issues: Improper weight initialization can result in symmetry among neurons in the same layer. When multiple neurons have the same weights and biases, they update in the same way during training, essentially behaving as a single neuron. This redundancy limits the learning capacity of the network, leading to suboptimal performance. Breaking this symmetry by carefully initializing the weights with random values allows each neuron to learn different features, enhancing the network's learning capabilities.\n",
    "\n",
    "Limited generalization ability: Weight initialization affects the generalization ability of the model, which refers to its performance on unseen data. If the weights are initialized inappropriately, the model may overfit the training data by memorizing specific patterns or noise, failing to generalize well to new examples. This issue can lead to poor performance on real-world data and reduced model reliability.\n",
    "\n",
    "Unstable learning dynamics: Improper weight initialization can introduce instability in the learning dynamics of the model. The model may exhibit erratic behavior during training, where the loss function fluctuates or diverges. This instability can hinder the model's ability to learn meaningful representations and make reliable predictions.\n",
    "\n",
    "To address these challenges, careful weight initialization techniques have been developed, such as Xavier initialization, He initialization, and variants thereof. These techniques aim to set the initial weights in a way that balances the gradients, breaks symmetry, and promotes stable and efficient learning dynamics, facilitating faster convergence and improved model performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925ccb7-fe2f-4956-b39f-cbf3a641cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Discuss the concept of variance and how it relates to weight initialization. WhE is it crucial to consider the\n",
    "variance of weights during initializationC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8467297-016a-4105-acf5-9dc7c3639b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variance is a statistical measure that quantifies the spread or dispersion of a set of values. In the context of weight initialization, variance refers to the spread of the initial weight values assigned to the connections between neurons in a neural network.\n",
    "\n",
    "The variance of weights during initialization is crucial for several reasons:\n",
    "\n",
    "Activation behavior: The variance of weights affects the activation behavior of neurons. Each neuron in a neural network computes a weighted sum of its inputs, which is then passed through an activation function. The spread of initial weight values determines the range of inputs that can activate the neuron. If the variance is too small, the neuron may have a limited activation range, leading to reduced learning capacity and suboptimal performance. Conversely, if the variance is too large, the neuron may become excessively sensitive to input variations, causing instability during training.\n",
    "\n",
    "Signal propagation: The variance of weights influences how signals propagate through the network during forward and backward passes. During forward propagation, the input signals are multiplied by the weights, and their spread determines the range of values that the subsequent layers receive. If the variance is too small, it can cause signal compression, where the information is squeezed into a narrow range. On the other hand, if the variance is too large, it can lead to signal explosion, where the values grow rapidly, making it difficult for subsequent layers to handle them effectively. Both scenarios can hinder efficient information flow and negatively impact model performance.\n",
    "\n",
    "Gradient flow: The variance of weights affects the magnitude of gradients during backpropagation. When updating the weights through gradient descent, the gradients are multiplied by the input signals to determine the weight update. If the variance is too small, it can result in small gradients, leading to slow convergence or vanishing gradients. Conversely, if the variance is too large, it can cause large gradients, resulting in unstable learning dynamics or exploding gradients. Proper weight initialization with an appropriate variance helps maintain a reasonable gradient magnitude, enabling stable and efficient learning.\n",
    "\n",
    "Avoiding saturation: Improper weight initialization can lead to saturation of activation functions. Saturation occurs when the input values to activation functions fall into the extreme regions where the gradients are close to zero. This can happen when the initial weight values push the activations towards these extreme regions. Saturation inhibits the learning process and slows down convergence. By considering the variance of weights during initialization, it is possible to prevent or minimize saturation, allowing the network to learn more effectively.\n",
    "\n",
    "In summary, the variance of weights during initialization has a significant impact on the behavior of neural networks. It affects the activation behavior, signal propagation, gradient flow, and the avoidance of saturation. By carefully considering the variance of weights, it is possible to set an appropriate initialization range that promotes stable learning dynamics, efficient information flow, and improved training performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3e82d-1500-45ea-96e2-33f80d283705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate\n",
    "to usek\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f2ed7-6d06-4e3f-9be2-53c84d5b2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zero initialization is a weight initialization technique where all the weights in a neural network are set to zero. It is a simple and intuitive approach, as it initializes the weights with the same value and removes any initial randomness. However, zero initialization has some limitations that need to be considered.\n",
    "\n",
    "One major limitation of zero initialization is that it leads to symmetric gradients during backpropagation. Since all the weights are initialized to the same value, the gradients of all the weights in a layer will also be the same. As a result, all the neurons in the layer will update their weights in the same way, causing them to learn the same features and making them redundant. This symmetry hampers the network's learning capacity, as it limits the diversity of learned representations.\n",
    "\n",
    "Another limitation is that zero initialization fails to break the symmetry between neurons. In neural networks, breaking symmetry is crucial for effective learning, as it allows different neurons to learn different features from the input data. Zero initialization does not provide this diversity, and as a result, the network may struggle to learn complex patterns and representations.\n",
    "\n",
    "Despite its limitations, zero initialization can be appropriate in certain situations:\n",
    "\n",
    "Bias initialization: Zero initialization is commonly used for initializing biases. Biases provide a shift or offset to the activations of neurons. Initializing biases to zero ensures that the neurons start with no bias and allows them to learn appropriate biases during training based on the data.\n",
    "\n",
    "Transfer learning: In transfer learning scenarios, where pre-trained weights are used as a starting point for a new task, zero initialization can be appropriate. By initializing the weights to zero and fine-tuning the pre-trained model, the network can retain the knowledge from the previous task while adapting to the new task. In this case, the pre-trained weights already capture relevant patterns, and zero initialization helps prevent interference with the learned representations.\n",
    "\n",
    "Sparse activation: Zero initialization can be effective for encouraging sparse activation in certain scenarios. Sparse activation refers to a situation where only a few neurons are activated for a given input, reducing the computational and memory requirements. By initializing the weights to zero, the network starts with a tendency towards sparse activation, and subsequent learning can reinforce this behavior.\n",
    "\n",
    "It's important to note that in most cases, using zero initialization for all the weights in a neural network is not recommended. Initialization techniques like Xavier initialization or He initialization, which introduce some randomness and are designed to address the limitations of zero initialization, are generally preferred as they provide better learning dynamics and help the network converge more effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9162753-c9f2-455b-a71d-2133d2617e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "k Describe the process of random initialization. How can random initialization be adjusted to mitigate\n",
    "potential issues like saturation or vanishing/exploding gradientsD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769247e-3243-461a-b71c-170545f73182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random initialization is a commonly used technique for initializing the weights in a neural network. Instead of setting all the weights to the same value, random initialization assigns random values to each weight, allowing for diversity and breaking symmetry among neurons. Here's a step-by-step process of random initialization:\n",
    "\n",
    "Determine the size and architecture of the neural network, including the number of layers and the number of neurons in each layer.\n",
    "\n",
    "Choose a probability distribution for generating the random values. Commonly used distributions include uniform, normal (Gaussian), or truncated normal distributions.\n",
    "\n",
    "Set the range or standard deviation of the distribution based on the activation function used in the network. Different activation functions have different ranges of effective gradients. For example, the ReLU activation function has an effective gradient range of [0, +∞), while the sigmoid activation function has a range of (0, 0.25]. The initialization process should be adjusted accordingly to ensure that the weights fall within these ranges.\n",
    "\n",
    "Initialize each weight in the network by drawing random values from the chosen distribution. The range of random values should be determined based on the activation function to avoid saturation or vanishing/exploding gradients.\n",
    "\n",
    "To mitigate potential issues like saturation, vanishing, or exploding gradients during random initialization, several adjustments can be made:\n",
    "\n",
    "Scaling based on activation function: Scaling the random initialization based on the activation function can help prevent saturation or gradient issues. For example, in the case of ReLU activation, the weights can be initialized using a Gaussian distribution with a smaller standard deviation, such as He initialization, which divides the random values by the square root of the number of inputs to the neuron.\n",
    "\n",
    "Leaky ReLU or variants: Instead of using the standard ReLU activation function, which can lead to dead neurons during initialization, using variants like Leaky ReLU or Parametric ReLU can provide a small gradient for negative inputs, mitigating the saturation problem.\n",
    "\n",
    "Initialization techniques like Xavier or He initialization: These techniques aim to adjust the random initialization based on the number of inputs and outputs of each neuron to ensure a balanced variance of the inputs. Xavier initialization scales the random values based on the number of inputs, while He initialization scales them based on both the number of inputs and outputs.\n",
    "\n",
    "Batch normalization: Applying batch normalization after weight initialization can help stabilize and normalize the activations throughout the network, reducing the chances of saturation or exploding gradients.\n",
    "\n",
    "Gradient clipping: During training, gradient clipping can be applied to limit the magnitude of gradients, preventing them from becoming too large or too small. This technique can help mitigate the exploding or vanishing gradients problem that can arise due to improper weight initialization.\n",
    "\n",
    "By adjusting the random initialization process as described above, it is possible to mitigate potential issues such as saturation, vanishing, or exploding gradients, providing better stability, faster convergence, and improved learning dynamics in the neural network.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18883148-a8f9-4dc5-a2dc-a3134f7d01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper\n",
    "weight initialization and the underlEing theorE behind itk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba161d3-1ee9-47fa-8fab-e8e01e79fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Xavier initialization, also known as Glorot initialization, is a widely used technique for weight initialization in neural networks. It addresses the challenges of improper weight initialization by considering the number of input and output connections of each neuron.\n",
    "\n",
    "The underlying theory behind Xavier initialization is based on maintaining a reasonable variance of the activations and gradients throughout the network, promoting stable and efficient learning dynamics. It achieves this by initializing the weights with random values that are scaled based on the number of inputs and outputs of the neuron.\n",
    "\n",
    "Xavier initialization is typically applied to weight matrices, where the weights are drawn from a distribution with zero mean and a variance that depends on the number of input and output connections. The variance of the distribution is calculated as:\n",
    "\n",
    "variance = 1 / (fan_in + fan_out)\n",
    "\n",
    "where fan_in represents the number of input connections to the neuron, and fan_out represents the number of output connections. The random values are then drawn from a distribution centered around zero with a standard deviation equal to the square root of the variance.\n",
    "\n",
    "The idea behind this initialization is to balance the variances of the input and output signals to each neuron. When the variance of the weights is too small, it can lead to vanishing gradients and slow convergence. Conversely, if the variance is too large, it can result in exploding gradients and instability during training. Xavier initialization ensures that the variance of the input and output signals to each neuron is approximately the same, allowing for efficient flow of information through the network.\n",
    "\n",
    "By considering both the number of input and output connections, Xavier initialization effectively addresses the issue of symmetry breaking. It prevents the neurons from learning redundant features by providing a diverse range of initial weights. This allows each neuron to learn different representations from the input data, enhancing the learning capacity and performance of the network.\n",
    "\n",
    "Xavier initialization is widely used in various activation functions, such as sigmoid and tanh, which have a symmetric activation range. It has been shown to improve the convergence speed and performance of neural networks, particularly in deep learning architectures where the vanishing and exploding gradients problem is more prevalent.\n",
    "\n",
    "Extensions of Xavier initialization, such as the He initialization, were later proposed to accommodate activation functions like ReLU, which have an asymmetric activation range. He initialization scales the variance by a factor of 2, taking into account the activation function's characteristics.\n",
    "\n",
    "In summary, Xavier initialization addresses the challenges of improper weight initialization by considering the number of input and output connections. By balancing the variances of the weights, it promotes stable learning dynamics, symmetry breaking, and efficient information flow in neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2da675-bec3-4e26-a79b-d4d38b4513a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement different weight initialization techniques (zero initialization, random initialization, Xavier\n",
    "initialization, and He initialization) in a neural network using a framework of Eour choice. Train the model\n",
    "on a suitable dataset and compare the performance of the initialized modelsk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0651bf0f-b4cd-4988-9996-634160f1961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=9dc3f3cf62be519460a975d263016733dabf48dfa49aaaac880dadb8a897a18d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/56/14/294a6c208bce35b0fc3170fe1049b2fd3f61ce6495fc3870b3\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision\n",
      "Successfully installed cmake-3.26.4 filelock-3.12.2 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d849b83-b62e-4be1-9351-8ba6bdbbf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f79a7c-5e3e-4530-8e1a-172ac90eefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b49726-bcaf-4f7d-bc0f-d4200c41b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 784\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = MNIST(root=\"./data\", train=False, transform=transform)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize models with different weight initialization techniques\n",
    "    models = {\n",
    "        \"Zero Initialization\": NeuralNetwork(input_size, hidden_size, output_size).to(device),\n",
    "        \"Random Initialization\": NeuralNetwork(input_size, hidden_size, output_size).to(device),\n",
    "        \"Xavier Initialization\": NeuralNetwork(input_size, hidden_size, output_size).to(device),\n",
    "        \"He Initialization\": NeuralNetwork(input_size, hidden_size, output_size).to(device)\n",
    "    }\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizers = {\n",
    "        \"Zero Initialization\": optim.SGD(models[\"Zero Initialization\"].parameters(), lr=learning_rate),\n",
    "        \"Random Initialization\": optim.SGD(models[\"Random Initialization\"].parameters(), lr=learning_rate),\n",
    "        \"Xavier Initialization\": optim.SGD(models[\"Xavier Initialization\"].parameters(), lr=learning_rate),\n",
    "        \"He Initialization\": optim.SGD(models[\"He Initialization\"].parameters(), lr=learning_rate)\n",
    "    }\n",
    "\n",
    "    # Train models\n",
    "    for epoch in range(num_epochs):\n",
    "        for model_name, model in models.items():\n",
    "            optimizer = optimizers[model_name]\n",
    "            train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Evaluate models\n",
    "        accuracies = {}\n",
    "        for model_name, model in models.items():\n",
    "            accuracy = evaluate(model, test_loader, device)\n",
    "            accuracies[model_name] = accuracy\n",
    "\n",
    "        # Print accuracy for each model\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        for model_name, accuracy in accuracies.items():\n",
    "            print(f\"{model_name}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00171a3-3042-48f5-9f7c-65988d90e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique\n",
    "for a given neural network architecture and task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de36842-20cd-4584-8cb0-cb4f26d003b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When choosing the appropriate weight initialization technique for a neural network, several considerations and tradeoffs need to be taken into account. The choice of weight initialization can have a significant impact on the learning dynamics, convergence speed, and overall performance of the network. Here are some considerations and tradeoffs to consider:\n",
    "\n",
    "Activation functions: Different weight initialization techniques are designed to work well with specific activation functions. For example, Xavier initialization is suitable for activation functions with symmetric activation ranges, such as sigmoid or tanh, while He initialization is better suited for activation functions like ReLU. Consider the activation functions used in your network and choose the weight initialization technique that aligns well with them.\n",
    "\n",
    "Network architecture and depth: The impact of weight initialization can be more pronounced in deeper neural networks. Deep networks are more prone to issues like vanishing or exploding gradients, and improper initialization can exacerbate these problems. Techniques like Xavier or He initialization, which take into account the network's size and depth, are often better suited for deep networks compared to simple random or zero initialization.\n",
    "\n",
    "Task complexity: The complexity of the task being performed by the neural network can influence the choice of weight initialization. For simple tasks with smaller datasets, simpler weight initialization techniques like random initialization may be sufficient. However, for more complex tasks with larger datasets, more sophisticated techniques like Xavier or He initialization can help achieve better performance and faster convergence.\n",
    "\n",
    "Dataset characteristics: The characteristics of the dataset, such as the distribution of the input data, can also influence the choice of weight initialization. If the dataset exhibits certain statistical properties or has imbalanced classes, specific initialization techniques may be more suitable. It can be beneficial to analyze the dataset and consider any specific requirements for weight initialization based on its properties.\n",
    "\n",
    "Computational resources: Some weight initialization techniques, like He initialization, tend to generate larger initial weights compared to others. This can result in increased memory and computational requirements, especially in large-scale neural networks. Consider the available computational resources and potential memory constraints when selecting a weight initialization technique.\n",
    "\n",
    "Empirical experimentation: While there are guidelines and best practices for weight initialization, it's important to experiment and compare different techniques on your specific network architecture and task. Empirical evaluation can help identify which initialization technique leads to better convergence, improved performance, and faster training for your specific scenario.\n",
    "\n",
    "In summary, choosing the appropriate weight initialization technique involves considering factors such as activation functions, network architecture, task complexity, dataset characteristics, computational resources, and empirical evaluation. It is important to strike a balance between initialization complexity, convergence speed, and overall performance, and adapt the choice of weight initialization to the specific requirements of your neural network and task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4be23c-d028-4907-880a-bf4a31d53e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd2264-d327-4613-8423-d7a57c614d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54f795-7451-4581-bf19-3180769dfa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e48e8a-bfc1-40e2-b2cb-f67befe3ed00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
