{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c496a-6cba-4657-8a94-e4772e4868a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6f014-fe87-421f-a21a-a9075fe15839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Polynomial functions and kernel functions are both used in machine learning algorithms for mapping input data into a higher-dimensional feature space. This is often done to make the data more separable so that a linear classifier can be used.\n",
    "\n",
    "Polynomial functions are a type of feature mapping where the input data is transformed into a higher-dimensional space using a polynomial function. For example, given a two-dimensional input (x, y), a degree-2 polynomial feature map would transform it into a six-dimensional space with features (1, x, y, x^2, xy, y^2).\n",
    "\n",
    "Kernel functions, on the other hand, are used in kernel methods such as Support Vector Machines (SVMs) to implicitly map the data into a higher-dimensional feature space without actually computing the transformation explicitly. The kernel function computes the dot product of the feature vectors in the higher-dimensional space without explicitly computing the coordinates of the vectors themselves. The most commonly used kernel functions are the linear kernel, polynomial kernel, and Gaussian kernel.\n",
    "\n",
    "In other words, polynomial functions are used as explicit feature mappings in some machine learning algorithms, while kernel functions are used as implicit feature mappings in other algorithms. Both are used to make the data more separable in a higher-dimensional space so that a linear classifier can be used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4ca31-8bf5-43ab-9b76-76928c5f6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f575aae1-20e8-4dd0-8479-bdb906d09d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier with a polynomial kernel\n",
    "svm_clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the SVM classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf0cc9-1726-4b5d-844c-aa7a02d18ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe55bd-26bc-4a05-8923-a741abc0fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In Support Vector Regression (SVR), epsilon is a hyperparameter that controls the width of the margin around the regression line. The margin is the region where no penalty is incurred on the regression error, and data points outside the margin are considered as errors and penalized.\n",
    "\n",
    "Increasing the value of epsilon in SVR increases the width of the margin. As the margin becomes wider, more data points can fit inside it without incurring a penalty on the regression error. This means that more data points can be classified as support vectors. Therefore, increasing the value of epsilon in SVR can increase the number of support vectors.\n",
    "\n",
    "However, it's important to note that increasing the number of support vectors can also increase the complexity of the model, which can lead to overfitting and decreased generalization performance on new data. Therefore, the value of epsilon should be chosen carefully to balance the trade-off between model complexity and performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5621fb-7cf7-44c6-ae00-35f855045ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770db31f-4e54-4dd0-8fca-0e867b67c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter can all significantly affect the performance of Support Vector Regression (SVR).\n",
    "\n",
    "Kernel function: The kernel function determines the mapping of the input data to a higher-dimensional space where the regression function can be constructed. Different kernel functions have different properties, and the choice of kernel function depends on the nature of the data and the task at hand. For example, the linear kernel is suitable for linearly separable data, while the RBF kernel is suitable for non-linear data with complex decision boundaries.\n",
    "\n",
    "C parameter: The C parameter controls the trade-off between the flatness of the regression line and the number of support vectors. A larger value of C allows for a more flexible regression line that can fit the training data more closely but may lead to overfitting. Conversely, a smaller value of C results in a flatter regression line with fewer support vectors and less risk of overfitting. In general, it's important to choose an appropriate value of C to balance the trade-off between model complexity and performance.\n",
    "\n",
    "Epsilon parameter: The epsilon parameter defines the width of the margin around the regression line. A larger value of epsilon results in a wider margin and fewer support vectors, while a smaller value of epsilon results in a narrower margin and more support vectors. Increasing epsilon can lead to a simpler model with higher bias and lower variance, while decreasing epsilon can lead to a more complex model with lower bias and higher variance.\n",
    "\n",
    "Gamma parameter: The gamma parameter controls the shape of the decision boundary and the influence of each training example on the regression line. A larger value of gamma results in a more complex decision boundary with a tighter fit to the training data and higher risk of overfitting. Conversely, a smaller value of gamma results in a smoother decision boundary with a lower risk of overfitting. The choice of gamma depends on the nature of the data and the complexity of the decision boundary.\n",
    "\n",
    "In general, there is no one-size-fits-all answer to the choice of these parameters, and the optimal values may vary depending on the specific dataset and the task at hand. It's important to experiment with different values of these parameters and use techniques such as cross-validation to evaluate their impact on the performance of the SVR model. For example, if the data is highly non-linear, you may want to use a non-linear kernel such as the RBF kernel and increase the gamma parameter to capture the complexity of the decision boundary. If the data is noisy or there are outliers, you may want to increase the value of epsilon to increase the robustness of the model. If the dataset is small, you may want to decrease the C parameter to prevent overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acbb82e-6b94-441d-969f-2d4136408a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans. 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0c03c-0acc-4368-8cdb-d577e871454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}\n",
    "grid_search = GridSearchCV(SVC(), params, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = grid_search.best_estimator_\n",
    "tuned_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'iris_svc_classifier.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
