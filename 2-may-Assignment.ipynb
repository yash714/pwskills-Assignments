{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062adeef-ce72-4fa5-8abc-d9429851efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebb2b8-397d-460b-9277-4ad073a6cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Anomaly detection is a technique used to identify patterns in data that are significantly different from the expected behavior or norm. The purpose of anomaly detection is to identify unusual data points, events, or observations that deviate from the expected pattern or baseline.\n",
    "\n",
    "Anomaly detection is used in a wide range of applications, such as fraud detection, intrusion detection in cybersecurity, network monitoring, system health monitoring, predictive maintenance, and quality control in manufacturing. In these applications, anomaly detection can help identify potential issues or anomalies early, allowing for timely interventions or corrective actions.\n",
    "\n",
    "Anomaly detection can be performed using various techniques, including statistical methods, machine learning algorithms, and rule-based approaches. These techniques analyze data and identify patterns that are different from the expected behavior or norm. The choice of technique depends on the specific application and the type of data being analyzed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9da26-a7e0-41b7-a9e4-47be88d57ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a775-2db6-4a46-8583-526088e2230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Anomaly detection is the process of identifying patterns in data that do not conform to expected behavior. While it has numerous applications in various fields, including finance, cybersecurity, and fraud detection, it also poses several challenges that can make it difficult to implement effectively. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "Lack of labeled data: One of the primary challenges of anomaly detection is that it requires labeled data to train machine learning models effectively. However, in many cases, labeled data may not be readily available, which can make it challenging to build accurate models.\n",
    "\n",
    "High false positive rates: Another significant challenge of anomaly detection is that it can be challenging to achieve a high level of accuracy while avoiding false positives. False positives occur when the system identifies an event as anomalous when it is, in fact, a normal occurrence. This can lead to a loss of trust in the system and wasted resources.\n",
    "\n",
    "Complexity of data: Anomaly detection often deals with complex, high-dimensional data, which can be difficult to analyze and identify patterns in. This can require the use of advanced statistical techniques and machine learning algorithms to identify meaningful anomalies.\n",
    "\n",
    "Concept drift: Anomaly detection models are trained on historical data, but the distribution of the data can change over time, leading to concept drift. This can result in models becoming less accurate and requiring frequent retraining to maintain their effectiveness.\n",
    "\n",
    "Adversarial attacks: In some applications, attackers may intentionally try to evade or confuse anomaly detection systems, making it challenging to detect anomalous behavior accurately.\n",
    "\n",
    "Overall, these challenges make anomaly detection a complex and challenging problem to solve, requiring advanced techniques and careful consideration of the specific application's needs and requirements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2b310-3adc-4b28-9d9e-ce3911c164c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03e5b6-d4c7-4ff3-9dce-afe3ffa3983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The primary difference between unsupervised anomaly detection and supervised anomaly detection lies in the availability of labeled data during the training phase. In supervised anomaly detection, labeled data is available to train the machine learning model, while in unsupervised anomaly detection, labeled data is not available, and the model must learn to detect anomalies based solely on the input data.\n",
    "\n",
    "Supervised Anomaly Detection:\n",
    "In supervised anomaly detection, the machine learning model is trained on a labeled dataset that contains both normal and anomalous data points. The model learns to distinguish between normal and anomalous data points based on the labels provided during the training phase. During the testing phase, the model is evaluated on a new set of data points, and anomalies are detected based on the classification result of the model. Supervised anomaly detection requires a labeled dataset, and the model's performance depends on the quality and representativeness of the labeled data.\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "In unsupervised anomaly detection, the machine learning model is trained on an unlabeled dataset that contains only normal data points. The model learns to identify patterns and regularities in the data and identify data points that deviate significantly from the learned patterns as anomalies. Unsupervised anomaly detection does not require labeled data, which makes it more flexible and applicable to a broader range of applications. However, it can also be more challenging to achieve high accuracy, especially when the anomalies are subtle or rare.\n",
    "\n",
    "In summary, supervised anomaly detection requires labeled data to train the model and is typically used when the anomalous data points are well defined and easy to label. Unsupervised anomaly detection, on the other hand, does not require labeled data and is more suitable for applications where anomalous data points are rare or difficult to label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6973a-07bd-4309-8cdf-641f55adecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0f4cf-ac49-477e-83aa-4de23603d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Anomaly detection algorithms can be broadly categorized into three main categories: statistical, machine learning, and hybrid approaches.\n",
    "\n",
    "Statistical approaches: Statistical approaches are based on the assumption that anomalies are rare occurrences that can be detected by looking at deviations from the normal distribution of the data. These approaches include techniques such as univariate and multivariate statistical models, time-series analysis, and control charts.\n",
    "\n",
    "Machine learning approaches: Machine learning approaches use algorithms that can learn from the data to detect anomalies. These approaches include techniques such as clustering, classification, and regression algorithms. They can be either supervised or unsupervised, depending on whether labeled data is available during training.\n",
    "\n",
    "Hybrid approaches: Hybrid approaches combine statistical and machine learning techniques to detect anomalies. These approaches leverage the strengths of both approaches to improve the accuracy of anomaly detection. For example, a hybrid approach may use a statistical technique to preprocess the data and extract features, followed by a machine learning algorithm to detect anomalies based on the extracted features.\n",
    "\n",
    "Within each category, there are many specific algorithms and techniques that can be used for anomaly detection. For example, some popular statistical techniques include the Z-score method and the Box-Cox transformation, while machine learning approaches can include techniques such as k-nearest neighbors, decision trees, and support vector machines. The choice of algorithm or technique depends on the specific requirements of the application and the characteristics of the data being analyzed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eacd5-5b02-44a7-b7ad-f4057fd63197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550bcbc-606e-45b5-b53f-ad96d3671db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distance-based anomaly detection methods are a class of unsupervised machine learning algorithms that identify anomalies based on the distance between data points. These methods rely on the assumption that normal data points are clustered together in the feature space, while anomalous data points are located far away from the normal clusters.\n",
    "\n",
    "The main assumptions made by distance-based anomaly detection methods are:\n",
    "\n",
    "Data points are represented as vectors in a high-dimensional feature space: Distance-based anomaly detection methods assume that each data point is represented as a vector in a high-dimensional feature space, where each feature represents a different aspect of the data.\n",
    "\n",
    "Normal data points are clustered together: Distance-based anomaly detection methods assume that normal data points are clustered together in the feature space, forming a dense region. This assumption is based on the intuition that normal data points should be similar to each other and differ significantly from anomalous data points.\n",
    "\n",
    "Anomalous data points are far away from the normal clusters: Distance-based anomaly detection methods assume that anomalous data points are located far away from the normal clusters, forming sparse regions in the feature space. This assumption is based on the intuition that anomalous data points are dissimilar to normal data points and should be separated from them by a large distance.\n",
    "\n",
    "The distribution of normal data points is stationary: Distance-based anomaly detection methods assume that the distribution of normal data points is stationary over time, meaning that it does not change significantly over time. This assumption is important because distance-based methods rely on a fixed threshold to separate normal and anomalous data points, and changes in the distribution can lead to inaccurate anomaly detection results.\n",
    "\n",
    "Overall, distance-based anomaly detection methods are based on the assumptions that normal data points are clustered together, anomalous data points are far away from the normal clusters, and the distribution of normal data points is stationary over time. These assumptions provide a useful framework for identifying anomalies based on the distance between data points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c57ab4-5d9f-4265-8a45-801c5528ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595fbb4-c197-450a-a3c6-81fedad22a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Local Outlier Factor (LOF) algorithm is an unsupervised machine learning algorithm used for anomaly detection. The LOF algorithm computes anomaly scores based on the local density of data points in the feature space.\n",
    "\n",
    "To compute the anomaly score of a data point using the LOF algorithm, the following steps are taken:\n",
    "\n",
    "Calculate the k-distance of the data point: The k-distance of a data point is defined as the distance from the data point to its k-th nearest neighbor. The value of k is a hyperparameter that needs to be set before applying the algorithm.\n",
    "\n",
    "Find the k-nearest neighbors of the data point: The k-nearest neighbors of a data point are defined as the k data points that are closest to the data point based on the distance metric used.\n",
    "\n",
    "Calculate the reachability distance of each neighbor: The reachability distance of a neighbor is defined as the maximum of the k-distance of the data point and the distance between the data point and its neighbor.\n",
    "\n",
    "Calculate the local reachability density of the data point: The local reachability density of a data point is defined as the inverse of the average reachability distance of its k-nearest neighbors. This value represents the local density of the data point in the feature space.\n",
    "\n",
    "Calculate the local outlier factor of the data point: The local outlier factor of a data point is defined as the ratio of the average local reachability density of its k-nearest neighbors to its own local reachability density. This value represents how much more or less dense the data point is compared to its neighbors. A value greater than 1 indicates that the data point is denser than its neighbors, while a value less than 1 indicates that the data point is less dense than its neighbors.\n",
    "\n",
    "Use the local outlier factor as the anomaly score: The local outlier factor computed for each data point can be used as the anomaly score for that data point. Data points with a higher local outlier factor are considered to be more anomalous.\n",
    "\n",
    "Overall, the LOF algorithm computes the anomaly score of a data point based on the local density of data points in the feature space. The algorithm uses the local outlier factor to quantify how anomalous a data point is relative to its neighbors, with higher values indicating greater anomalousness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5537fa2-0f5a-40d4-a529-bd70b718c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15c371-6eba-497b-b9de-428420ac9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Isolation Forest algorithm is an unsupervised machine learning algorithm used for anomaly detection. It is based on the concept of randomly partitioning the data into isolation trees to identify anomalies. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "n_estimators: The number of isolation trees to build. Increasing the number of trees can improve the accuracy of the algorithm but also increases the computation time and memory requirements.\n",
    "\n",
    "max_samples: The number of samples to use when building each isolation tree. This parameter controls the degree of subsampling performed on the data. By default, the value is set to \"auto\", which means that the algorithm uses the entire dataset to build each tree.\n",
    "\n",
    "contamination: The percentage of data points that are expected to be anomalous. This parameter controls the threshold for identifying anomalies. A lower value of contamination will result in a more conservative approach to anomaly detection, while a higher value will result in more data points being labeled as anomalies.\n",
    "\n",
    "max_features: The maximum number of features to consider when splitting each node in an isolation tree. This parameter controls the degree of feature subsampling performed on the data. By default, the value is set to \"auto\", which means that the algorithm uses all features to split each node.\n",
    "\n",
    "bootstrap: Whether to use bootstrap sampling when building each isolation tree. Bootstrap sampling is a resampling method that randomly selects samples with replacement from the dataset. This parameter controls the degree of randomness introduced in the algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd8fd6-3fc4-4f9d-b2bf-f5c8e69bb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708758c-6842-45b1-930b-468e84fbb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To compute the anomaly score using KNN with K=10, we need to calculate the local density and reachability distance of the data point based on its 10 nearest neighbors. However, in this case, the data point has only 2 neighbors of the same class within a radius of 0.5, which means that it does not have 10 neighbors. Therefore, we cannot compute the anomaly score using KNN with K=10 for this data point.\n",
    "\n",
    "In such cases, it is common to use a lower value of K or a different algorithm that can handle sparse or low-density regions in the data. For example, the Local Outlier Factor (LOF) algorithm can handle sparse regions by computing the local density of each data point based on its k-nearest neighbors, where k is a hyperparameter that can be set based on the density of the data. The LOF algorithm can be used to compute the anomaly score of the data point in this scenario.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc263d-0045-4118-b987-b4d3fb9494b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792085d0-a33c-4119-9592-f8c1f0aec9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The anomaly score for a data point using the Isolation Forest algorithm is calculated based on the average path length of the data point across all the isolation trees in the forest. The intuition is that an anomaly is more likely to have a shorter average path length than a normal point since it can be isolated more easily.\n",
    "\n",
    "The average path length of a data point is defined as the average number of edges in the shortest path from the root node to the terminal node that isolates the data point in each isolation tree. The average path length for a normal data point is expected to be higher than that of an anomaly.\n",
    "\n",
    "To calculate the anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees in an Isolation Forest with 100 trees and a dataset of 3000 data points, we need to compute the normalized path length score as follows:\n",
    "\n",
    "Compute the average path length of all data points across all the trees in the forest.\n",
    "\n",
    "Let the average path length of all data points across all trees be denoted by E(h(x)).\n",
    "\n",
    "Compute the standard deviation of the path lengths of all data points across all the trees in the forest.\n",
    "\n",
    "Let the standard deviation of the path lengths of all data points across all trees be denoted by std(h(x)).\n",
    "\n",
    "Compute the path length of the data point in each tree.\n",
    "\n",
    "Let the average path length of the data point across all trees be denoted by h(x).\n",
    "\n",
    "Compute the normalized path length score of the data point\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c565a98-0957-429b-b16a-720f60a860cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
