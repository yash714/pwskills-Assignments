{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b29002-14bb-45b7-a8fc-a38e74686359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd54e7-a882-403b-becd-500cbc45b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Missing values in a dataset refer to the absence of a particular value for a variable in a particular observation or data point. Missing values can occur due to various reasons such as data entry errors, incomplete data, or problems with data collection processes.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can negatively impact the accuracy and reliability of the data analysis. If missing values are not handled appropriately, they can lead to biased results, incorrect statistical inference, and inaccurate predictive models.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "Decision Trees: Decision tree algorithms can handle missing values by finding the next best split in the tree based on the available data.\n",
    "\n",
    "Random Forests: Random forest algorithms can handle missing values by imputing them with the median value of the corresponding feature or using surrogate splits to replace missing values.\n",
    "\n",
    "XGBoost: XGBoost algorithm can handle missing values by automatically learning how to treat them during the tree building process.\n",
    "\n",
    "Support Vector Machines (SVM): SVMs can handle missing values by ignoring the missing data points during the optimization process.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN algorithm can handle missing values by imputing them with the mean value of the corresponding feature or using the most common value.\n",
    "\n",
    "There are several methods to handle missing values such as imputation, deletion, and prediction. The choice of method depends on the amount of missing data, the type of data, and the goals of the analysis.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c62d01-25b5-4448-9b99-91c44e1b5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada3202-88bf-41b7-955f-6c4ef2635846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "There are several techniques used to handle missing data in a dataset. Some of the commonly used techniques are:\n",
    "\n",
    "Deletion: Deletion involves removing the rows or columns that contain missing values. Deletion can be further classified into three types:\n",
    "\n",
    "a. Listwise Deletion: This involves deleting all the rows that contain missing values.\n",
    "\n",
    "b. Pairwise Deletion: This involves retaining only the observations that have values for the variables of interest.\n",
    "\n",
    "c. Column Deletion: This involves deleting the entire column if it contains a significant number of missing values.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# listwise deletion\n",
    "df1 = df.dropna()\n",
    "print(df1)\n",
    "\n",
    "# pairwise deletion\n",
    "df2 = df[['A', 'C']].dropna()\n",
    "print(df2)\n",
    "\n",
    "# column deletion\n",
    "df3 = df.dropna(axis=1)\n",
    "print(df3)\n",
    "\n",
    "\n",
    "Imputation: Imputation involves replacing the missing values with some other values. There are various techniques for imputation such as mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# mean imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df1 = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Prediction: Prediction involves using a model to predict the missing values based on the available data. This method is useful when the missing values are correlated with other variables in the dataset.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# prediction using random forest regressor\n",
    "df1 = df.copy()\n",
    "for col in df1.columns:\n",
    "    if df1[col].isnull().sum() > 0:\n",
    "        X_train = df1[df1[col].notnull()].drop(col, axis=1)\n",
    "        y_train = df1[df1[col].notnull()][col]\n",
    "        X_test = df1[df1[col].isnull()].drop(col, axis=1)\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        df1.loc[df1[col].isnull(), col] = rf.predict(X_test)\n",
    "print(df1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad872fe-f145-4e7c-af81-531d7370a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f818b-ac9a-457b-8e73-3369f76991a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Imbalanced data refers to a situation where the distribution of classes in the dataset is not equal, and one or more classes have significantly fewer samples than the others. In other words, in imbalanced data, one class is underrepresented compared to the other classes.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models that perform poorly on the underrepresented class. The model will tend to favor the majority class because it has more samples and can achieve higher accuracy by simply predicting the majority class for most instances. This can result in a significant decrease in the performance of the model on the minority class, leading to poor predictions, false negatives, or missed opportunities.\n",
    "\n",
    "For example, consider a medical diagnosis dataset where the prevalence of a rare disease is 1%. If the model is trained on this dataset without handling the imbalanced data, it is likely to predict negative for all instances, leading to a high false-negative rate, which is dangerous for the patient's health.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data to ensure that the model is not biased and performs well on all classes, including the minority class. There are several techniques to handle imbalanced data, such as oversampling, undersampling, cost-sensitive learning, and synthetic minority oversampling technique (SMOTE).\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa4ca0-940c-41bc-ad68-2707d73af969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955d8d4-76d6-47d7-9446-a67ed65aaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Up-sampling and down-sampling are two techniques used to handle imbalanced data in machine learning.\n",
    "\n",
    "Up-sampling refers to the process of increasing the number of samples in the minority class to balance the dataset with the majority class. This can be achieved by duplicating the minority class samples or generating synthetic samples using techniques like SMOTE.\n",
    "\n",
    "For example, consider a dataset of credit card transactions where only 1% of the transactions are fraudulent. To balance the dataset, we can up-sample the fraudulent transactions by duplicating them or generating synthetic transactions using SMOTE.\n",
    "\n",
    "Down-sampling, on the other hand, refers to the process of reducing the number of samples in the majority class to balance the dataset with the minority class. This can be achieved by randomly selecting a subset of the majority class samples or using more sophisticated techniques like Tomek links.\n",
    "\n",
    "For example, consider a dataset of customer churn where 90% of the customers do not churn. To balance the dataset, we can down-sample the non-churn customers by randomly selecting a subset of them.\n",
    "\n",
    "Both up-sampling and down-sampling can be used to handle imbalanced data, depending on the specific scenario. Up-sampling is useful when the minority class is small, and generating synthetic samples is possible without causing overfitting. Down-sampling is useful when the majority class is large, and removing some samples does not significantly affect the model's performance.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f92528-3c4b-4649-85ed-e86770a80a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7eddff-2af4-4dd1-839a-645175b149ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Data augmentation is a technique used in machine learning to increase the amount of training data by creating new, synthetic samples from the existing data. This is particularly useful in scenarios where the amount of available training data is limited, or when the model needs to be robust to variations in the input data.\n",
    "\n",
    "One popular technique for data augmentation in imbalanced datasets is SMOTE, which stands for Synthetic Minority Over-sampling Technique. SMOTE works by generating synthetic samples for the minority class by interpolating between existing samples in the minority class.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "Select a sample from the minority class\n",
    "Identify its k nearest neighbors in the minority class\n",
    "Choose one of the k neighbors at random\n",
    "Generate a new sample by interpolating between the selected sample and the chosen neighbor\n",
    "Repeat steps 1-4 until the desired number of new samples is generated\n",
    "By repeating this process for multiple samples in the minority class, SMOTE can generate a synthetic dataset that is balanced with the majority class. The new samples are not exact copies of the original samples, but rather are created by interpolating between existing samples in the minority class, resulting in a more diverse and representative dataset.\n",
    "\n",
    "SMOTE is a powerful technique for handling imbalanced data, but it should be used with caution. In some cases, generating too many synthetic samples can lead to overfitting, especially if the synthetic samples are very similar to the original samples. Therefore, it's important to carefully tune the parameters of SMOTE and evaluate its effectiveness on a validation set before using it on the test set.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5f611-280f-4234-9667-1ebbf415cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ced602-c5c7-4e3a-867e-49a740f77def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Outliers are data points that are significantly different from the other data points in the dataset. Outliers can be caused by measurement errors, data entry errors, or legitimate extreme values that are not representative of the overall data distribution.\n",
    "\n",
    "Handling outliers is essential because they can have a significant impact on the analysis and modeling of the dataset. Outliers can affect the mean and standard deviation of the dataset, which can in turn affect other statistical measures such as correlations, regression coefficients, and hypothesis tests. Outliers can also affect the performance of machine learning models, especially models that are sensitive to the distribution of the data.\n",
    "\n",
    "For example, consider a dataset of student test scores. If the dataset contains outliers, such as a score of 1000, this can skew the mean and standard deviation of the dataset and affect the accuracy of statistical measures such as the correlation between test scores and grades. In machine learning, outliers can cause overfitting, where the model learns to fit the outliers instead of the underlying pattern in the data, leading to poor generalization on new data.\n",
    "\n",
    "Therefore, it is important to identify and handle outliers in a dataset. There are several techniques for handling outliers, such as removing the outliers, replacing the outliers with a more typical value, or transforming the data to reduce the impact of outliers. However, it's important to carefully consider the impact of these techniques on the overall analysis and modeling of the dataset, and to evaluate their effectiveness on a validation set before applying them to the test set.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b29134-3f8f-4943-9e59-fdeab87f13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11adfbb-85e1-48b2-a506-abef77cc5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "There are several techniques that can be used to handle missing data in an analysis. Some of these techniques are:\n",
    "\n",
    "Deletion: This involves removing the rows or columns that contain missing values from the dataset. This technique can be used when the amount of missing data is small relative to the size of the dataset, and the missing data is missing at random. However, this technique can lead to a loss of information and reduced statistical power.\n",
    "\n",
    "Imputation: This involves filling in the missing values with estimated values based on the other values in the dataset. There are several imputation techniques, such as mean imputation, median imputation, and regression imputation. Mean imputation involves filling in the missing values with the mean of the other values in the same column. Median imputation involves filling in the missing values with the median of the other values in the same column. Regression imputation involves using a regression model to predict the missing values based on the other variables in the dataset.\n",
    "\n",
    "Multiple imputation: This involves creating multiple imputed datasets, each with a different set of estimated values for the missing data, and analyzing each dataset separately. The results are then combined to obtain an overall estimate. This technique can account for the uncertainty in the imputed values and provide more accurate estimates.\n",
    "\n",
    "Model-based imputation: This involves using a statistical model to estimate the missing values based on the other variables in the dataset. This technique can provide more accurate estimates than simple imputation methods but requires more computational resources.\n",
    "\n",
    "The choice of technique depends on the amount and pattern of missing data, the distribution of the data, and the goals of the analysis. It is important to carefully evaluate the impact of the chosen technique on the overall analysis and modeling of the dataset, and to evaluate its effectiveness on a validation set before applying it to the test set.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303ba91-e3cd-42a9-928f-d03e32273af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f495056-0af2-41e7-9e70-4f0cf6a00838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "There are several strategies that can be used to determine if missing data is missing at random or if there is a pattern to the missing data. Some of these strategies are:\n",
    "\n",
    "Visual inspection: One way to identify patterns in missing data is to visualize the data using plots or graphs. For example, you can create a heatmap of missing data to identify if there is a pattern to the missing values.\n",
    "\n",
    "Statistical tests: There are several statistical tests that can be used to test for missingness patterns. For example, the Little's MCAR test can be used to test if missing data is missing completely at random, and the Missing Indicator test can be used to test if missing data is missing at random.\n",
    "\n",
    "Imputation: Another strategy is to impute the missing data using a variety of techniques, and then to compare the results obtained from different imputation techniques. If the results obtained from different imputation techniques are similar, then it is likely that the missing data is missing at random.\n",
    "\n",
    "Domain knowledge: It is important to have domain knowledge of the dataset and the context in which the data was collected. This can help identify patterns in the missing data that may not be apparent from statistical tests or visual inspection.\n",
    "\n",
    "It is important to note that no single strategy is perfect, and a combination of these strategies may be necessary to determine if the missing data is missing at random or if there is a pattern to the missing data.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52630cbc-7152-4449-b445-6b0230163219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d2bfe-d1ed-46cc-ba5f-63e27aca8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "When working with imbalanced datasets, there are several strategies that can be used to evaluate the performance of a machine learning model. Some of these strategies are:\n",
    "\n",
    "Confusion Matrix: The confusion matrix is a table that is used to evaluate the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. The metrics that can be derived from the confusion matrix include accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Precision-Recall Curve: The precision-recall curve is a plot of precision versus recall at different probability thresholds. It is a useful tool for evaluating the performance of a classifier on imbalanced datasets.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve: The ROC curve is a plot of the true positive rate versus the false positive rate at different probability thresholds. It is a useful tool for evaluating the performance of a classifier on imbalanced datasets.\n",
    "\n",
    "Resampling Techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset before training the machine learning model. This can improve the performance of the model on the minority class.\n",
    "\n",
    "Cost-Sensitive Learning: Cost-sensitive learning is a technique that assigns different misclassification costs to different classes. This can be useful for improving the performance of a classifier on imbalanced datasets.\n",
    "\n",
    "It is important to note that no single strategy is perfect, and a combination of these strategies may be necessary to evaluate the performance of a machine learning model on imbalanced datasets.\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a137a-7cb2-4e00-93ba-ed9b42138a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b17cc2-43b8-4117-9017-1bcf8e5353de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "When dealing with an imbalanced dataset, particularly in a scenario where we want to estimate customer satisfaction, there are several methods that can be employed to balance the dataset and down-sample the majority class. Some of these methods are:\n",
    "\n",
    "Under-sampling: Under-sampling is a technique that involves removing some data points from the majority class to balance the dataset. One of the common ways to perform under-sampling is to randomly select a subset of data points from the majority class to match the size of the minority class. This technique can be useful when we have a large dataset with a majority class that is much larger than the minority class.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.satisfaction == 'satisfied']\n",
    "df_minority = df[df.satisfaction == 'dissatisfied']\n",
    "\n",
    "# Down-sample the majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,     # sample without replacement\n",
    "                                   n_samples=len(df_minority),    # to match minority class\n",
    "                                   random_state=42)   # reproducible results\n",
    "\n",
    "# Combine minority class and down-sampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "\n",
    "\n",
    "Over-sampling: Over-sampling is a technique that involves creating synthetic data points in the minority class to match the size of the majority class. One of the common ways to perform over-sampling is to use SMOTE (Synthetic Minority Over-sampling Technique), which creates synthetic data points by interpolating between existing data points.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "X = df.drop('satisfaction', axis=1)\n",
    "y = df['satisfaction']\n",
    "\n",
    "# Apply SMOTE over-sampling\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3d89e-c3bf-4a10-81f3-0ec19c5114a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fd41c-881f-4563-8f08-ec285935dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "When dealing with a dataset that has a low percentage of occurrences of a rare event, some methods that can be employed to balance the dataset and up-sample the minority class are:\n",
    "\n",
    "Random oversampling: This method involves randomly duplicating instances from the minority class to increase its representation in the dataset.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): This method creates synthetic examples from the minority class by interpolating between existing examples.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X and y are your feature matrix and target vector\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "After using the SMOTE algorithm, the minority class will be up-sampled to have a similar representation as the majority class in the dataset.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab5b18-0d90-4f8c-abb1-7b5e916e2f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1761e83-ee56-4dcb-8c96-20d1b9261a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e245c1-2963-4dfd-bf6d-35f39ec2b7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17468d-37b8-424d-aa7b-13228048dad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f01cfa-9a78-446e-80b7-2ebc98483181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fb75d-bb9b-4514-8571-9cdecd91b586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfb0eb-c505-4ac8-b8e7-07dafaa75d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637ac4c-04ba-4872-81ee-62135abdf29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
